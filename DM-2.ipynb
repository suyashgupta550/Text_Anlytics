{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c29b15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea86a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### IMPORTING ALL THE NECESSARY LIBRARIES \n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b20cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6314e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download the WordNet corpus if not already downloaded\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e221880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dfdb358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bdd5d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\asus\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90dbb471",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTING THE DATASET\n",
    "\n",
    "df = pd.read_excel(\"A_II_Emotion_Data_Student_Copy_Final.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26f6d49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_</th>\n",
       "      <th>brand_name_</th>\n",
       "      <th>country_</th>\n",
       "      <th>star_rating_</th>\n",
       "      <th>emotions_</th>\n",
       "      <th>text_reviews_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1</td>\n",
       "      <td>Z_</td>\n",
       "      <td>US</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fast shipping! All cloths was finally fits wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID10</td>\n",
       "      <td>Z_</td>\n",
       "      <td>GB</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just wanted to say how delighted I was with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID100</td>\n",
       "      <td>H_</td>\n",
       "      <td>FRI</td>\n",
       "      <td>5</td>\n",
       "      <td>joy</td>\n",
       "      <td>My order took 6 days with a snow day and a wee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID1000</td>\n",
       "      <td>H_</td>\n",
       "      <td>GB</td>\n",
       "      <td>1</td>\n",
       "      <td>fear</td>\n",
       "      <td>Wouldnt give them no stars. I ordered a coat 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID1001</td>\n",
       "      <td>H_</td>\n",
       "      <td>GB</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parcel never arrived. Chasing for a refund for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID_ brand_name_ country_  star_rating_ emotions_  \\\n",
       "0     ID1          Z_       US             5       NaN   \n",
       "1    ID10          Z_       GB             5       NaN   \n",
       "2   ID100          H_      FRI             5       joy   \n",
       "3  ID1000          H_       GB             1      fear   \n",
       "4  ID1001          H_       GB             1       NaN   \n",
       "\n",
       "                                       text_reviews_  \n",
       "0  Fast shipping! All cloths was finally fits wel...  \n",
       "1  Just wanted to say how delighted I was with th...  \n",
       "2  My order took 6 days with a snow day and a wee...  \n",
       "3  Wouldnt give them no stars. I ordered a coat 4...  \n",
       "4  Parcel never arrived. Chasing for a refund for...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2076ea2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5722, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "966cbb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Fast shipping! All cloths was finally fits wel...\n",
       "1    Just wanted to say how delighted I was with th...\n",
       "2    My order took 6 days with a snow day and a wee...\n",
       "3    Wouldnt give them no stars. I ordered a coat 4...\n",
       "4    Parcel never arrived. Chasing for a refund for...\n",
       "Name: text_reviews_, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_reviews_\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f07e822",
   "metadata": {},
   "source": [
    "### LOWER CASE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a8992b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONVERTING THE TEXT REVIEW COLUMN INTO LOWER CASE \n",
    "# Assuming 'df' is your DataFrame and 'review' is the column containing text reviews\n",
    "df['text_reviews_'] = df['text_reviews_'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d144f805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       fast shipping! all cloths was finally fits wel...\n",
       "1       just wanted to say how delighted i was with th...\n",
       "2       my order took 6 days with a snow day and a wee...\n",
       "3       wouldnt give them no stars. i ordered a coat 4...\n",
       "4       parcel never arrived. chasing for a refund for...\n",
       "                              ...                        \n",
       "5717    iãâ¯ãâ¾ãæãâ£ãæãâ£ãâ¯ãâ¾ãæãâ¯ãâ½ã...\n",
       "5718    i wanted to change the delivery location of my...\n",
       "5719    waiting since 24/14/2022. i believe the sent m...\n",
       "5720    who do you complain to about late deliveries f...\n",
       "5721    placed an order on the 24th november. h_ alert...\n",
       "Name: text_reviews_, Length: 5722, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_reviews_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f30e83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def check_for_punctuation(text):\n",
    "    # Define a regular expression pattern to match punctuation and special symbols\n",
    "    pattern = r'[\\W_]'  # Matches any non-word character or underscore\n",
    "\n",
    "    # Search for the pattern in the text\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'review' is the column containing text reviews\n",
    "df['punctuation'] = df['text_reviews_'].apply(check_for_punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0eea9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    [ , !,  ,  ,  ,  ,  ,  , !,  ,  , !]\n",
       "1       [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...\n",
       "2       [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...\n",
       "3       [ ,  ,  ,  , .,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...\n",
       "4       [ ,  , .,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , -, ...\n",
       "                              ...                        \n",
       "5717    [, ¯, , , , , £, , , , £, , ¯, , , ...\n",
       "5718    [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...\n",
       "5719    [ ,  , /, /, .,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...\n",
       "5720    [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ?, ?,  ,  , ...\n",
       "5721    [ ,  ,  ,  ,  ,  , .,  , _,  ,  ,  ,  ,  ,  , ...\n",
       "Name: punctuation, Length: 5722, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['punctuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ce72f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def check_for_irrelevant_characters(text):\n",
    "    # Define a regular expression pattern to match irrelevant characters\n",
    "    pattern = r'[^a-zA-Z0-9\\s]'  # Matches any character that is not a letter, digit, or whitespace\n",
    "\n",
    "    # Search for the pattern in the text\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'review' is the column containing text reviews\n",
    "df['irrelevant_characters'] = df['text_reviews_'].apply(check_for_irrelevant_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6141a9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               [!, !, !]\n",
       "1                                            [_, ., ., .]\n",
       "2                                                     [.]\n",
       "3                                         [., ., ., ., .]\n",
       "4                                            [., -, ., .]\n",
       "                              ...                        \n",
       "5717    [ã, , â, ¯, ã, , â, ¾, ã, , æ, , ã, , â, ...\n",
       "5718    [., ., ã, , â, ¯, ã, , â, ¾, ã, , æ, , ã, ...\n",
       "5719                                      [/, /, ., ., .]\n",
       "5720                                      [?, ?, ., ., !]\n",
       "5721                                [., _, ., &, ., ., .]\n",
       "Name: irrelevant_characters, Length: 5722, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['irrelevant_characters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a0fc1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    # Define a regular expression pattern to match irrelevant characters\n",
    "    pattern = r'[^a-zA-Z0-9\\s]'  # Matches any character that is not a letter, digit, or whitespace\n",
    "    punctuation_pattern = r'[{}]'.format(re.escape(string.punctuation))  # Matches any punctuation character\n",
    "    \n",
    "    # Concatenate all characters in the list into a single string\n",
    "    text = ''.join(text)\n",
    "    \n",
    "    # Remove irrelevant characters and punctuation using the patterns\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    cleaned_text = re.sub(punctuation_pattern, '', cleaned_text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'text' is the column containing lists of characters\n",
    "df['cleaned_text'] = df['text_reviews_'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1af2e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and you've already cleaned the text and stored it in the 'cleaned_text' column\n",
    "df['text_reviews_'] = df['cleaned_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7e4c7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       fast shipping all cloths was finally fits well...\n",
       "1       just wanted to say how delighted i was with th...\n",
       "2       my order took 6 days with a snow day and a wee...\n",
       "3       wouldnt give them no stars i ordered a coat 4 ...\n",
       "4       parcel never arrived chasing for a refund for ...\n",
       "                              ...                        \n",
       "5717    id have given 0 stars if i could if id have kn...\n",
       "5718    i wanted to change the delivery location of my...\n",
       "5719    waiting since 24142022 i believe the sent my o...\n",
       "5720    who do you complain to about late deliveries f...\n",
       "5721    placed an order on the 24th november h alerts ...\n",
       "Name: text_reviews_, Length: 5722, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_reviews_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0c706da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5712    h x evri welcome to hellupdate 3 i called h cu...\n",
      "5713    wating for wife to finish shopping   the worke...\n",
      "5714    first time i ever ordered it took weeks to com...\n",
      "5715    i received an item that was the wrong colour a...\n",
      "5716    missing delivery with evri for over a week and...\n",
      "5717    id have given 0 stars if i could if id have kn...\n",
      "5718    i wanted to change the delivery location of my...\n",
      "5719    waiting since 24142022 i believe the sent my o...\n",
      "5720    who do you complain to about late deliveries f...\n",
      "5721    placed an order on the 24th november h alerts ...\n",
      "Name: text_reviews_, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df' is your DataFrame\n",
    "last_rows_text = df.iloc[-10:]['text_reviews_']  # Assuming you want to see the last 10 rows\n",
    "print(last_rows_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df147ce",
   "metadata": {},
   "source": [
    "## To check if the text_review has removed the punctuation and irrelevant character "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5235af59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All punctuation and irrelevant characters have been successfully removed.\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df' is your DataFrame and 'cleaned_text' is the column containing the cleaned text\n",
    "original_text = df['text_reviews_']\n",
    "cleaned_text = df['cleaned_text']\n",
    "\n",
    "# Check if original_text and cleaned_text are equal\n",
    "all_characters_removed = original_text.equals(cleaned_text)\n",
    "\n",
    "if all_characters_removed:\n",
    "    print(\"All punctuation and irrelevant characters have been successfully removed.\")\n",
    "else:\n",
    "    print(\"Some punctuation or irrelevant characters may still be present.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8531aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text']= df['text_reviews_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "171dda5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    fast shipping all cloths was finally fits well...\n",
       "1    just wanted to say how delighted i was with th...\n",
       "2    my order took 6 days with a snow day and a wee...\n",
       "3    wouldnt give them no stars i ordered a coat 4 ...\n",
       "4    parcel never arrived chasing for a refund for ...\n",
       "Name: text_reviews_, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_reviews_'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "440fd68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and 'cleaned_text' is the column containing the cleaned text\n",
    "original_text = df['text_reviews_'].iloc[5712:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0189a81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5712    h x evri welcome to hellupdate 3 i called h cu...\n",
       "5713    wating for wife to finish shopping   the worke...\n",
       "5714    first time i ever ordered it took weeks to com...\n",
       "5715    i received an item that was the wrong colour a...\n",
       "5716    missing delivery with evri for over a week and...\n",
       "5717    id have given 0 stars if i could if id have kn...\n",
       "5718    i wanted to change the delivery location of my...\n",
       "5719    waiting since 24142022 i believe the sent my o...\n",
       "5720    who do you complain to about late deliveries f...\n",
       "5721    placed an order on the 24th november h alerts ...\n",
       "Name: text_reviews_, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0309d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID_', 'brand_name_', 'country_', 'star_rating_', 'emotions_',\n",
       "       'text_reviews_', 'punctuation', 'irrelevant_characters',\n",
       "       'cleaned_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82082d",
   "metadata": {},
   "source": [
    "## REMOVING THE UNECESSARY COLUMN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0e16069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame\n",
    "columns_to_remove = ['punctuation', 'irrelevant_characters', 'cleaned_text']\n",
    "df.drop(columns=columns_to_remove, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd03e16",
   "metadata": {},
   "source": [
    "## TOKENIZATION \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f49e969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the punkt tokenizer model if not already downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Apply tokenization to the 'text' column\n",
    "df['tokenized_text'] = df['text_reviews_'].apply(tokenize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e32d3247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID_', 'brand_name_', 'country_', 'star_rating_', 'emotions_',\n",
       "       'text_reviews_', 'tokenized_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5914f5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [fast, shipping, all, cloths, was, finally, fi...\n",
       "1       [just, wanted, to, say, how, delighted, i, was...\n",
       "2       [my, order, took, 6, days, with, a, snow, day,...\n",
       "3       [wouldnt, give, them, no, stars, i, ordered, a...\n",
       "4       [parcel, never, arrived, chasing, for, a, refu...\n",
       "                              ...                        \n",
       "5717    [id, have, given, 0, stars, if, i, could, if, ...\n",
       "5718    [i, wanted, to, change, the, delivery, locatio...\n",
       "5719    [waiting, since, 24142022, i, believe, the, se...\n",
       "5720    [who, do, you, complain, to, about, late, deli...\n",
       "5721    [placed, an, order, on, the, 24th, november, h...\n",
       "Name: tokenized_text, Length: 5722, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dbe552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c8795ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [fast, shipping, all, cloths, was, finally, fi...\n",
       "1    [just, wanted, to, say, how, delighted, i, was...\n",
       "2    [my, order, took, 6, days, with, a, snow, day,...\n",
       "3    [wouldnt, give, them, no, stars, i, ordered, a...\n",
       "4    [parcel, never, arrived, chasing, for, a, refu...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a9e487",
   "metadata": {},
   "source": [
    "## STOPWORDS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "249f3fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Getting the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stop words from tokenized text\n",
    "def remove_stop_words(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Apply stop word removal to the 'tokenized_text' column\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(remove_stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea1010d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [fast, shipping, cloths, finally, fits, well, ...\n",
       "1       [wanted, say, delighted, friendly, helpful, st...\n",
       "2       [order, took, 6, days, snow, day, weekend, hap...\n",
       "3       [wouldnt, give, stars, ordered, coat, 4, weeks...\n",
       "4       [parcel, never, arrived, chasing, refund, subs...\n",
       "                              ...                        \n",
       "5717    [id, given, 0, stars, could, id, known, evri, ...\n",
       "5718    [wanted, change, delivery, location, item, wit...\n",
       "5719    [waiting, since, 24142022, believe, sent, orde...\n",
       "5720    [complain, late, deliveries, horrendous, compn...\n",
       "5721    [placed, order, 24th, november, h, alerts, ord...\n",
       "Name: tokenized_text, Length: 5722, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524147c",
   "metadata": {},
   "source": [
    "## LEMMATISATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ec0c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to perform lemmatization on a list of tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Apply lemmatization to the 'tokenized_text' column\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lemmatize_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9e6ce7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [fast, shipping, cloth, finally, fit, well, sa...\n",
       "1       [wanted, say, delighted, friendly, helpful, st...\n",
       "2       [order, took, 6, day, snow, day, weekend, happ...\n",
       "3       [wouldnt, give, star, ordered, coat, 4, week, ...\n",
       "4       [parcel, never, arrived, chasing, refund, subs...\n",
       "                              ...                        \n",
       "5717    [id, given, 0, star, could, id, known, evri, h...\n",
       "5718    [wanted, change, delivery, location, item, wit...\n",
       "5719    [waiting, since, 24142022, believe, sent, orde...\n",
       "5720    [complain, late, delivery, horrendous, compnay...\n",
       "5721    [placed, order, 24th, november, h, alert, orde...\n",
       "Name: tokenized_text, Length: 5722, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "622df6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_</th>\n",
       "      <th>brand_name_</th>\n",
       "      <th>country_</th>\n",
       "      <th>star_rating_</th>\n",
       "      <th>emotions_</th>\n",
       "      <th>text_reviews_</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1</td>\n",
       "      <td>Z_</td>\n",
       "      <td>US</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fast shipping all cloths was finally fits well...</td>\n",
       "      <td>[fast, shipping, cloth, finally, fit, well, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID10</td>\n",
       "      <td>Z_</td>\n",
       "      <td>GB</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just wanted to say how delighted i was with th...</td>\n",
       "      <td>[wanted, say, delighted, friendly, helpful, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID100</td>\n",
       "      <td>H_</td>\n",
       "      <td>FRI</td>\n",
       "      <td>5</td>\n",
       "      <td>joy</td>\n",
       "      <td>my order took 6 days with a snow day and a wee...</td>\n",
       "      <td>[order, took, 6, day, snow, day, weekend, happ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID1000</td>\n",
       "      <td>H_</td>\n",
       "      <td>GB</td>\n",
       "      <td>1</td>\n",
       "      <td>fear</td>\n",
       "      <td>wouldnt give them no stars i ordered a coat 4 ...</td>\n",
       "      <td>[wouldnt, give, star, ordered, coat, 4, week, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID1001</td>\n",
       "      <td>H_</td>\n",
       "      <td>GB</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>parcel never arrived chasing for a refund for ...</td>\n",
       "      <td>[parcel, never, arrived, chasing, refund, subs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID_ brand_name_ country_  star_rating_ emotions_  \\\n",
       "0     ID1          Z_       US             5       NaN   \n",
       "1    ID10          Z_       GB             5       NaN   \n",
       "2   ID100          H_      FRI             5       joy   \n",
       "3  ID1000          H_       GB             1      fear   \n",
       "4  ID1001          H_       GB             1       NaN   \n",
       "\n",
       "                                       text_reviews_  \\\n",
       "0  fast shipping all cloths was finally fits well...   \n",
       "1  just wanted to say how delighted i was with th...   \n",
       "2  my order took 6 days with a snow day and a wee...   \n",
       "3  wouldnt give them no stars i ordered a coat 4 ...   \n",
       "4  parcel never arrived chasing for a refund for ...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [fast, shipping, cloth, finally, fit, well, sa...  \n",
       "1  [wanted, say, delighted, friendly, helpful, st...  \n",
       "2  [order, took, 6, day, snow, day, weekend, happ...  \n",
       "3  [wouldnt, give, star, ordered, coat, 4, week, ...  \n",
       "4  [parcel, never, arrived, chasing, refund, subs...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "072ee464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'joy', 'fear', 'surprise', 'anger', 'neutral', 'disgust',\n",
       "       'sadness'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emotions_'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31e2a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame\n",
    "df.drop(columns=['text_reviews_'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a69116d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_</th>\n",
       "      <th>brand_name_</th>\n",
       "      <th>country_</th>\n",
       "      <th>star_rating_</th>\n",
       "      <th>emotions_</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1</td>\n",
       "      <td>Z_</td>\n",
       "      <td>US</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast, shipping, cloth, finally, fit, well, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID10</td>\n",
       "      <td>Z_</td>\n",
       "      <td>GB</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[wanted, say, delighted, friendly, helpful, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID100</td>\n",
       "      <td>H_</td>\n",
       "      <td>FRI</td>\n",
       "      <td>5</td>\n",
       "      <td>joy</td>\n",
       "      <td>[order, took, 6, day, snow, day, weekend, happ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID1000</td>\n",
       "      <td>H_</td>\n",
       "      <td>GB</td>\n",
       "      <td>1</td>\n",
       "      <td>fear</td>\n",
       "      <td>[wouldnt, give, star, ordered, coat, 4, week, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID1001</td>\n",
       "      <td>H_</td>\n",
       "      <td>GB</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[parcel, never, arrived, chasing, refund, subs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID_ brand_name_ country_  star_rating_ emotions_  \\\n",
       "0     ID1          Z_       US             5       NaN   \n",
       "1    ID10          Z_       GB             5       NaN   \n",
       "2   ID100          H_      FRI             5       joy   \n",
       "3  ID1000          H_       GB             1      fear   \n",
       "4  ID1001          H_       GB             1       NaN   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [fast, shipping, cloth, finally, fit, well, sa...  \n",
       "1  [wanted, say, delighted, friendly, helpful, st...  \n",
       "2  [order, took, 6, day, snow, day, weekend, happ...  \n",
       "3  [wouldnt, give, star, ordered, coat, 4, week, ...  \n",
       "4  [parcel, never, arrived, chasing, refund, subs...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee47aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89422c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.40.0)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a290b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29cffdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_emotion(tokenized_text):\n",
    "    # Assuming 'tokenized_text' is the column containing tokenized text in your DataFrame df\n",
    "    model = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\")  # download pre-trained emotion classification model\n",
    "    label_score = model(tokenized_text.tolist())  # Convert the column to a list and pass it to the model\n",
    "    return label_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee8c440",
   "metadata": {},
   "source": [
    "## SEMI SUPERVISED LEARNING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d365fdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0aaa94fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_</th>\n",
       "      <th>brand_name_</th>\n",
       "      <th>country_</th>\n",
       "      <th>star_rating_</th>\n",
       "      <th>emotions_</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1</td>\n",
       "      <td>Z_</td>\n",
       "      <td>US</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast, shipping, cloth, finally, fit, well, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID10</td>\n",
       "      <td>Z_</td>\n",
       "      <td>GB</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[wanted, say, delighted, friendly, helpful, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID100</td>\n",
       "      <td>H_</td>\n",
       "      <td>FRI</td>\n",
       "      <td>5</td>\n",
       "      <td>joy</td>\n",
       "      <td>[order, took, 6, day, snow, day, weekend, happ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID1000</td>\n",
       "      <td>H_</td>\n",
       "      <td>GB</td>\n",
       "      <td>1</td>\n",
       "      <td>fear</td>\n",
       "      <td>[wouldnt, give, star, ordered, coat, 4, week, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID1001</td>\n",
       "      <td>H_</td>\n",
       "      <td>GB</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[parcel, never, arrived, chasing, refund, subs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID_ brand_name_ country_  star_rating_ emotions_  \\\n",
       "0     ID1          Z_       US             5       NaN   \n",
       "1    ID10          Z_       GB             5       NaN   \n",
       "2   ID100          H_      FRI             5       joy   \n",
       "3  ID1000          H_       GB             1      fear   \n",
       "4  ID1001          H_       GB             1       NaN   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [fast, shipping, cloth, finally, fit, well, sa...  \n",
       "1  [wanted, say, delighted, friendly, helpful, st...  \n",
       "2  [order, took, 6, day, snow, day, weekend, happ...  \n",
       "3  [wouldnt, give, star, ordered, coat, 4, week, ...  \n",
       "4  [parcel, never, arrived, chasing, refund, subs...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db976051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [tokenized_text, emotions_]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "unlabeled_data = df[df['emotions_'] == 'NaN'][['tokenized_text']]\n",
    "unlabeled_data['emotions_'] = -1\n",
    "print(unlabeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ad5d39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         tokenized_text  emotions_\n",
      "0     [fast, shipping, cloth, finally, fit, well, sa...         -1\n",
      "1     [wanted, say, delighted, friendly, helpful, st...         -1\n",
      "4     [parcel, never, arrived, chasing, refund, subs...         -1\n",
      "5     [avoid, ordering, online, ordered, 12, day, ag...         -1\n",
      "6     [awful, service, returning, item, booked, coll...         -1\n",
      "...                                                 ...        ...\n",
      "5717  [id, given, 0, star, could, id, known, evri, h...         -1\n",
      "5718  [wanted, change, delivery, location, item, wit...         -1\n",
      "5719  [waiting, since, 24142022, believe, sent, orde...         -1\n",
      "5720  [complain, late, delivery, horrendous, compnay...         -1\n",
      "5721  [placed, order, 24th, november, h, alert, orde...         -1\n",
      "\n",
      "[5095 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Filter rows with NaN values in the 'emotions_' column and select the 'tokenized_text' column\n",
    "unlabeled_data = df[pd.isna(df['emotions_'])][['tokenized_text']]\n",
    "\n",
    "# Assign the label -1 to the 'emotions_' column for unlabeled data\n",
    "unlabeled_data['emotions_'] = -1\n",
    "\n",
    "print(unlabeled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e1f6340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labeled data as data where \"Sentiment\" is not missing\n",
    "labeled_data = df[df['emotions_'].notna() & (df['emotions_'] != 'NaN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0ed33f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_</th>\n",
       "      <th>brand_name_</th>\n",
       "      <th>country_</th>\n",
       "      <th>star_rating_</th>\n",
       "      <th>emotions_</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID100</td>\n",
       "      <td>H_</td>\n",
       "      <td>FRI</td>\n",
       "      <td>5</td>\n",
       "      <td>joy</td>\n",
       "      <td>[order, took, 6, day, snow, day, weekend, happ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID1000</td>\n",
       "      <td>H_</td>\n",
       "      <td>GB</td>\n",
       "      <td>1</td>\n",
       "      <td>fear</td>\n",
       "      <td>[wouldnt, give, star, ordered, coat, 4, week, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ID1007</td>\n",
       "      <td>H_</td>\n",
       "      <td>GB</td>\n",
       "      <td>1</td>\n",
       "      <td>surprise</td>\n",
       "      <td>[went, h, glasgow, saturday, bought, kid, stuf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ID1014</td>\n",
       "      <td>H_</td>\n",
       "      <td>GB</td>\n",
       "      <td>1</td>\n",
       "      <td>anger</td>\n",
       "      <td>[trying, order, product, online, cost, option,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ID1020</td>\n",
       "      <td>H_</td>\n",
       "      <td>GB</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[use, evri]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5692</th>\n",
       "      <td>ID972</td>\n",
       "      <td>H_</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[dont, order, online, go, store, tooooo, much,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5695</th>\n",
       "      <td>ID975</td>\n",
       "      <td>H_</td>\n",
       "      <td>GB</td>\n",
       "      <td>1</td>\n",
       "      <td>fear</td>\n",
       "      <td>[h, use, evri, formally, hermes, courier, lost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5701</th>\n",
       "      <td>ID980</td>\n",
       "      <td>H_</td>\n",
       "      <td>GB</td>\n",
       "      <td>1</td>\n",
       "      <td>anger</td>\n",
       "      <td>[staff, westfield, stratford, rude, went, 2, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5711</th>\n",
       "      <td>ID99</td>\n",
       "      <td>H_</td>\n",
       "      <td>DE</td>\n",
       "      <td>5</td>\n",
       "      <td>joy</td>\n",
       "      <td>[packet, came, weekim, super, satisfiedeveryth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5715</th>\n",
       "      <td>ID993</td>\n",
       "      <td>H_</td>\n",
       "      <td>GB</td>\n",
       "      <td>1</td>\n",
       "      <td>anger</td>\n",
       "      <td>[received, item, wrong, colour, damaged, conta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>627 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID_ brand_name_ country_  star_rating_ emotions_  \\\n",
       "2      ID100          H_      FRI             5       joy   \n",
       "3     ID1000          H_       GB             1      fear   \n",
       "10    ID1007          H_       GB             1  surprise   \n",
       "18    ID1014          H_       GB             1     anger   \n",
       "25    ID1020          H_       GB             1   neutral   \n",
       "...      ...         ...      ...           ...       ...   \n",
       "5692   ID972          H_       US             1   sadness   \n",
       "5695   ID975          H_       GB             1      fear   \n",
       "5701   ID980          H_       GB             1     anger   \n",
       "5711    ID99          H_       DE             5       joy   \n",
       "5715   ID993          H_       GB             1     anger   \n",
       "\n",
       "                                         tokenized_text  \n",
       "2     [order, took, 6, day, snow, day, weekend, happ...  \n",
       "3     [wouldnt, give, star, ordered, coat, 4, week, ...  \n",
       "10    [went, h, glasgow, saturday, bought, kid, stuf...  \n",
       "18    [trying, order, product, online, cost, option,...  \n",
       "25                                          [use, evri]  \n",
       "...                                                 ...  \n",
       "5692  [dont, order, online, go, store, tooooo, much,...  \n",
       "5695  [h, use, evri, formally, hermes, courier, lost...  \n",
       "5701  [staff, westfield, stratford, rude, went, 2, 5...  \n",
       "5711  [packet, came, weekim, super, satisfiedeveryth...  \n",
       "5715  [received, item, wrong, colour, damaged, conta...  \n",
       "\n",
       "[627 rows x 6 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "73adc77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels from labeled_data\n",
    "y_labeled = labeled_data['emotions_']\n",
    "y_unlabeled = unlabeled_data['emotions_']\n",
    "X_labeled = labeled_data['tokenized_text']\n",
    "X_unlabeled = unlabeled_data['tokenized_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62c8cbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_labeled  2            joy\n",
      "3           fear\n",
      "10      surprise\n",
      "18         anger\n",
      "25       neutral\n",
      "          ...   \n",
      "5692     sadness\n",
      "5695        fear\n",
      "5701       anger\n",
      "5711         joy\n",
      "5715       anger\n",
      "Name: emotions_, Length: 627, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"y_labeled \",y_labeled )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b73a574b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_unlabeled 0      -1\n",
      "1      -1\n",
      "4      -1\n",
      "5      -1\n",
      "6      -1\n",
      "       ..\n",
      "5717   -1\n",
      "5718   -1\n",
      "5719   -1\n",
      "5720   -1\n",
      "5721   -1\n",
      "Name: emotions_, Length: 5095, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"y_unlabeled\",y_unlabeled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04c2b8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_unlabeled 0       [fast, shipping, cloth, finally, fit, well, sa...\n",
      "1       [wanted, say, delighted, friendly, helpful, st...\n",
      "4       [parcel, never, arrived, chasing, refund, subs...\n",
      "5       [avoid, ordering, online, ordered, 12, day, ag...\n",
      "6       [awful, service, returning, item, booked, coll...\n",
      "                              ...                        \n",
      "5717    [id, given, 0, star, could, id, known, evri, h...\n",
      "5718    [wanted, change, delivery, location, item, wit...\n",
      "5719    [waiting, since, 24142022, believe, sent, orde...\n",
      "5720    [complain, late, delivery, horrendous, compnay...\n",
      "5721    [placed, order, 24th, november, h, alert, orde...\n",
      "Name: tokenized_text, Length: 5095, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"X_unlabeled\",X_unlabeled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "85579892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_labeled 2       [order, took, 6, day, snow, day, weekend, happ...\n",
      "3       [wouldnt, give, star, ordered, coat, 4, week, ...\n",
      "10      [went, h, glasgow, saturday, bought, kid, stuf...\n",
      "18      [trying, order, product, online, cost, option,...\n",
      "25                                            [use, evri]\n",
      "                              ...                        \n",
      "5692    [dont, order, online, go, store, tooooo, much,...\n",
      "5695    [h, use, evri, formally, hermes, courier, lost...\n",
      "5701    [staff, westfield, stratford, rude, went, 2, 5...\n",
      "5711    [packet, came, weekim, super, satisfiedeveryth...\n",
      "5715    [received, item, wrong, colour, damaged, conta...\n",
      "Name: tokenized_text, Length: 627, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"X_labeled\",X_labeled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cea50dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ID_ brand_name_ country_  star_rating_ emotions_  \\\n",
      "0        ID1          Z_       US             5       NaN   \n",
      "1       ID10          Z_       GB             5       NaN   \n",
      "2      ID100          H_      FRI             5       joy   \n",
      "3     ID1000          H_       GB             1      fear   \n",
      "4     ID1001          H_       GB             1       NaN   \n",
      "...      ...         ...      ...           ...       ...   \n",
      "5717   ID995          H_       GB             1       NaN   \n",
      "5718   ID996          H_       GB             1       NaN   \n",
      "5719   ID997          H_       CY             1       NaN   \n",
      "5720   ID998          H_       GB             1       NaN   \n",
      "5721   ID999          H_       NL             1       NaN   \n",
      "\n",
      "                                         tokenized_text  \n",
      "0     [fast, shipping, cloth, finally, fit, well, sa...  \n",
      "1     [wanted, say, delighted, friendly, helpful, st...  \n",
      "2     [order, took, 6, day, snow, day, weekend, happ...  \n",
      "3     [wouldnt, give, star, ordered, coat, 4, week, ...  \n",
      "4     [parcel, never, arrived, chasing, refund, subs...  \n",
      "...                                                 ...  \n",
      "5717  [id, given, 0, star, could, id, known, evri, h...  \n",
      "5718  [wanted, change, delivery, location, item, wit...  \n",
      "5719  [waiting, since, 24142022, believe, sent, orde...  \n",
      "5720  [complain, late, delivery, horrendous, compnay...  \n",
      "5721  [placed, order, 24th, november, h, alert, orde...  \n",
      "\n",
      "[5722 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29eea759",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PIPELINE\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Parameters\n",
    "sdg_params = dict(alpha=1e-5, penalty=\"l2\", loss=\"log_loss\")\n",
    "vectorizer_params = dict(ngram_range=(1, 2), min_df=1, max_df=0.8)\n",
    "\n",
    "# Supervised Pipeline\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer(**vectorizer_params)),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"clf\", SGDClassifier(**sdg_params)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# LabelSpreading Pipeline\n",
    "ls_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer(**vectorizer_params)),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        # LabelSpreading does not support dense matrices\n",
    "        (\"toarray\", FunctionTransformer(lambda x: x.toarray())),\n",
    "        (\"clf\", LabelSpreading(kernel='rbf', gamma=20, n_neighbors=7, alpha=0.2, max_iter=30, tol=0.001, n_jobs=None)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb4f32",
   "metadata": {},
   "source": [
    "## Define a function for a classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "07db661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_and_print_metrics(clf, X_train, y_train, X_test, y_test):\n",
    "    print(\"Number of training samples:\", len(X_train))\n",
    "    print(\"Unlabeled samples in training set:\", sum(1 for x in y_train if x == -1)) #if x == 'NaN'\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    #print(\"y Train\", y_train)\n",
    "    #print(\"y Predict\",y_pred)\n",
    "    #print(\"y Test\",y_test)\n",
    "\n",
    "    print(\n",
    "        \"Micro-averaged F1 score on test set: %0.3f\"\n",
    "        % f1_score(y_test, y_pred, average=\"micro\")\n",
    "    )\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred,zero_division=1))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "31d237e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "def eval_and_print_metrics(clf, X_train, y_train, X_test, y_test):\n",
    "    print(\"Number of training samples:\", len(X_train))\n",
    "    print(\"Unlabeled samples in training set:\", sum(1 for x in y_train if x == -1)) #if x == 'NaN'\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(\n",
    "        \"Micro-averaged F1 score on test set: %0.3f\"\n",
    "        % f1_score(y_test, y_pred, average=\"micro\")\n",
    "    )\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred,zero_division=1))\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "# Example usage:\n",
    "# eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0bef11a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc04b3ac",
   "metadata": {},
   "source": [
    "## SPLITTING THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "de9078a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_labeled, y_labeled, test_size=0.2, stratify=y_labeled, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a990009",
   "metadata": {},
   "source": [
    "## Supervised SGDClassifier on the labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "664abf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised SGDClassifier on the labeled data:\n",
      "Number of training samples: 501\n",
      "Unlabeled samples in training set: 0\n",
      "Micro-averaged F1 score on test set: 0.540\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 4  1  0  1  2  1  2]\n",
      " [ 1  7  1  1  1  3  2]\n",
      " [ 1  1  8  1  0  3  2]\n",
      " [ 0  0  2 13  0  3  2]\n",
      " [ 1  1  1  0 13  1  3]\n",
      " [ 2  2  0  1  6  8  1]\n",
      " [ 0  3  0  1  1  3 15]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.44      0.36      0.40        11\n",
      "     disgust       0.47      0.44      0.45        16\n",
      "        fear       0.67      0.50      0.57        16\n",
      "         joy       0.72      0.65      0.68        20\n",
      "     neutral       0.57      0.65      0.60        20\n",
      "     sadness       0.36      0.40      0.38        20\n",
      "    surprise       0.56      0.65      0.60        23\n",
      "\n",
      "    accuracy                           0.54       126\n",
      "   macro avg       0.54      0.52      0.53       126\n",
      "weighted avg       0.55      0.54      0.54       126\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert lists of tokens to strings\n",
    "X_train = X_train.apply(lambda x: ' '.join(x))\n",
    "X_test = X_test.apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Supervised SGDClassifier on the labeled data\n",
    "print(\"Supervised SGDClassifier on the labeled data:\")\n",
    "eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78ffbc",
   "metadata": {},
   "source": [
    "### Label Spreading Algorithm on the labeled and unlabeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f9d573",
   "metadata": {},
   "source": [
    "## Manage Labeled and Unlabeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d55cc037",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = X_test.index\n",
    "#print(\"TEST INDICES\",test_indices)\n",
    "\n",
    "# Exclude test data from X_labeled and y_labeled based on the identified indices\n",
    "X_labeled_filtered = X_labeled.drop(index=test_indices, errors='ignore')\n",
    "y_labeled_filtered = y_labeled.drop(index=test_indices, errors='ignore')\n",
    "\n",
    "# Concatenate the filtered labeled data with the unlabeled data\n",
    "X=X_combined = pd.concat([X_labeled_filtered, X_unlabeled])\n",
    "y=y_combined = pd.concat([y_labeled_filtered, y_unlabeled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3574f998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     [order, took, 6, day, snow, day, weekend, happ...\n",
       "10    [went, h, glasgow, saturday, bought, kid, stuf...\n",
       "18    [trying, order, product, online, cost, option,...\n",
       "25                                          [use, evri]\n",
       "31    [worst, customer, service, delivery, worse, am...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "75b06f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2          joy\n",
       "10    surprise\n",
       "18       anger\n",
       "25     neutral\n",
       "31     disgust\n",
       "Name: emotions_, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2dfd05",
   "metadata": {},
   "source": [
    "## MAPPING LABELS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9f722055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Spreading Classifier on the labeled and unlabeled data:\n",
      "Micro-averaged F1 score on test set: 0.18253968253968253\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 0  0 20  0  0  0  0]\n",
      " [ 0  0 16  0  0  0  0]\n",
      " [ 0  0 23  0  0  0  0]\n",
      " [ 0  0 11  0  0  0  0]\n",
      " [ 0  0 20  0  0  0  0]\n",
      " [ 0  0 16  0  0  0  0]\n",
      " [ 0  0 20  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# Convert tokenized text back to text\n",
    "X_text = [' '.join(tokens) for tokens in X]\n",
    "X_test_text = [' '.join(tokens) for tokens in X_test]\n",
    "\n",
    "# Fit the LabelSpreading pipeline on the combined labeled and unlabeled data\n",
    "ls_pipeline.fit(X_text, y)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_ls = ls_pipeline.predict(X_test_text)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Label Spreading Classifier on the labeled and unlabeled data:\")\n",
    "print(\"Micro-averaged F1 score on test set:\", f1_score(y_test, y_pred_ls, average='micro'))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_ls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fb44bd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Spreading Classifier on the labeled and unlabeled data:\n",
      "Micro-averaged F1 score on test set: 0.18253968253968253\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        20\n",
      "           2       0.00      0.00      0.00        16\n",
      "           3       0.18      1.00      0.31        23\n",
      "           4       0.00      0.00      0.00        11\n",
      "           5       0.00      0.00      0.00        20\n",
      "           6       0.00      0.00      0.00        16\n",
      "           7       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.18       126\n",
      "   macro avg       0.03      0.14      0.04       126\n",
      "weighted avg       0.03      0.18      0.06       126\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 0  0 20  0  0  0  0]\n",
      " [ 0  0 16  0  0  0  0]\n",
      " [ 0  0 23  0  0  0  0]\n",
      " [ 0  0 11  0  0  0  0]\n",
      " [ 0  0 20  0  0  0  0]\n",
      " [ 0  0 16  0  0  0  0]\n",
      " [ 0  0 20  0  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Fit the LabelSpreading pipeline on the combined labeled and unlabeled data\n",
    "ls_pipeline.fit(X_text, y)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_ls = ls_pipeline.predict(X_test_text)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Label Spreading Classifier on the labeled and unlabeled data:\")\n",
    "print(\"Micro-averaged F1 score on test set:\", f1_score(y_test, y_pred_ls, average='micro'))\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_ls))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_ls))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73afaf3e",
   "metadata": {},
   "source": [
    "# VISUALISATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efcc271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb3c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed471bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09234e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
